--f
title: "Initial Analysis"
output:
  workflowr::wflow_html:
    toc: false
    code_folding: hide
---


```{r results='hide', message=FALSE, warning=FALSE, cache=TRUE}
options(warn=-1)

source('lib/functions.R')
source('lib/get_data.R')

library(tsibble)
library(lubridate)
library(sp)
#library(tidyquant)
opts_chunk$set(cache=TRUE, autodep=TRUE)

read.csv('data/EveningMasterFullAnonym.csv') %>% 
  as.tibble %>% 
{ . } -> df_all
```

# 

```{r read_data }

my_db_read( 'select * from location') %>% 
  as.tibble() %>% 
  { . } ->  df_location

my_db_read( 'select * from passivelocation') %>% 
  as.tibble() %>% 
  { . } ->  df_passive_location
 
```

# how clean is the gps data - Plan
  * for each person / night, what is the range of distance between successive gps points
  * what are the different attributes of gps data (passive/active, etc)

# Data preperation
## merge in passive location
passive location is just as good as active location (but mostly duplicated, we will come to that later

```{r merge}

df_location %<>% 
  bind_rows( df_passive_location ) %>%
  mutate( id=row_number() )

```

## data cleaning
1) eliminate duplicates - many duplicate locations at a time stamp. Keep most accurate location at a time stamp
2) many locations at north pole, markedly wrong.  Eliminate them.
3) calculate interval between timestamps







df_filtered_location %>%
  filter(userid=='f181ac9f-f678-40ce-89ea-7d5c807e3b68' & night == '2014-10-18') %>%
  filter( dist_filtered > 10) %>%
  select( id ) %$% id -> b

```{r  data_cleaning}
library(geosphere)

distance2centroid = function( df ) {
  # distance from last point to centroid
  distm(
    colMeans( df  %>% select( latitude, longitude )),
    df %>% slice(n()) %>% select( latitude, longitude ),
    fun = distHaversine
  )

}


findStayPoint = function (df, max_jump_time, min_staypoint_time, max_staypoint_distance) {

  n_staypoint = 0  # 0 not in, 1:n which staypoint
  df$n_staypoint = n_staypoint
  n_staypoint = n_staypoint + 1
  in_staypoint=FALSE
  nrow = nrow( df )
  start=1
  end=1

  while ( end <= nrow ) {

    if ( end <= start ) {
      end = end + 1
      next
    }

    last_duration = df[ end, ]$time_stamp - df[ end-1, ]$time_stamp
    entire_duration = df[ end, ]$time_stamp - df[ start, ]$time_stamp

    #cat( paste( start, end, 'last_duration', last_duration , 'entire_duration',    entire_duration,in_staypoint,"\n"))

    if ( last_duration >= max_jump_time )   {

        # we have too much time between this point and the last point
        # leave untagged points outside staypoint
        start = end
        if (in_staypoint ){
          n_staypoint = n_staypoint + 1
          in_staypoint = FALSE
        }
        next
    }

    # is this point a staypoint
    d = distance2centroid( slice( df, start:end ))
    #cat( paste( d, "\n"))

    if ( d > max_staypoint_distance ) {

      # we have physically moved out of the previous staypoint zone, start:end
      if ( in_staypoint ){

        # this is a mark, keep the previous staypoint, start afresh
        n_staypoint = n_staypoint + 1
        in_staypoint = FALSE
        start=end
      } else {

        # keep looking, move the staypoint zone forward
        start = start + 1
      }
      next
    }

    # we are within staypoint distance of centroid( start:end )
    if ((entire_duration >= min_staypoint_time ) ) {

      # we have been in staypoint sufficient time
      df[ start:end,]$n_staypoint = n_staypoint
      in_staypoint = TRUE
    }
    end = end + 1
  }

df
}

min_staypoint_time=20*60
max_jump_time=20*60
max_staypoint_distance=200



df_location %>%
  mutate( is_broken_reading = ifelse( longitude < 0 | longitude >10 | latitude <40, 1, 0),
         night = as.factor(night)) %>%
  arrange( userid, night, local_time, is_broken_reading, accuracy ) %>%
  group_by( userid, night, local_time ) %>%
  filter( id == min(id ) ) %>% 
  ungroup() %>% 
  filter( is_broken_reading == 0 ) %>% 
  select( -is_broken_reading ) %>% 
  group_by( userid, night ) %>%
  filter( length(local_time) > 1 ) %>%
  mutate( interval = difference( local_time, lag=1 )) %>% 
  ungroup() %>% 
  { . } -> df_best_location

df_best_location %>% 
  filter( userid==.[1,]$userid & night==.[1,]$night  ) %>% 
  { . } -> b

df_best_location %>% 
  group_by( userid, night ) %>%
  group_modify( ~findStayPoint(.x,  max_jump_time, min_staypoint_time, max_staypoint_distance)) %>%
{ . } -> df_staypoints

```

# calculate distance 
  - calculate distance between successive best guess locations
  - calculate speed based on interval and distance, in m/sec 


```{r distance}

calc_interval_distance = function( longitude, latitude ) {
  c( longitude, latitude ) %>%
    matrix( ncol = 2 ) %>%
    spDists( segments=TRUE, longlat=TRUE) %>%
    c(0,.)
}

 
calc_distance_from_start = function( longitude, latitude ) {
  c( longitude, latitude ) %>%
    matrix( ncol = 2 ) %>%
    spDistsN1(., .[1,], longlat=TRUE) 
}

df_best_location %>% 
  group_by( night ) %>%
  mutate( dist = calc_interval_distance(longitude, latitude),
         speed = dist/interval * 1000) %>%  # in m/sec
  select( interval, dist, speed, accuracy, bearing, everything())  %>% 
  { . } -> df_filtered_location

```
# Inaccurate GPS investigation
Sometimes, the person 'moves' very quickly;  for example, many samples where speed > 100.  This has to be related to measurement issues

GPS has problems.  There are several solutions
  1. delete out of bounds measurements
  1. average successive points (eg moving average)
  1. use median of successive points (https://gis.stackexchange.com/a/245009/70843)
  1. eliminate changes in location where accellerometer clearly states that the phone is not moving
  1. Calculate Euclidean minimum spanning tree of points:
  1. use a kalman filter, which would use more of teh  sensor data.  Difficult. Here is an impllementation for android (https://blog.maddevs.io/reduce-gps-data-error-on-android-with-kalman-filter-and-accelerometer-43594faed19c)

For this initial work, I use the median 7 filter


```{r dataFiltering}


df_best_location %>%
  group_by( userid, night ) %>%
  mutate( 
         latitude_median = rollapplyr( latitude, 11, median, partial = TRUE ),
         longitude_median = rollapplyr( longitude, 11, median, partial = TRUE ),
#         dist_original = calc_interval_distance(longitude, latitude),
#         dist_cumul_original = calc_distance_from_start(latitude, longitude),
#         speed_original = dist_original/interval * 1000,   # in m/sec
         dist_filtered = calc_interval_distance(latitude_median, longitude_median ),
         dist_cumul_filtered = calc_distance_from_start(latitude_median, longitude_median ),
         speed_filtered = dist_filtered/interval * 1000) %>%  # in m/sec
  select( interval, dist_filtered, speed_filtered, accuracy, bearing, everything())  %>% 
    ungroup() %>%
    { . } -> df_filtered_location



```

# Distance sanity checking
## any points greater than 10km from previous point?

```{r DistanceTesting}
  
df_filtered_location %>% filter( dist_filtered >10 ) %>% head(10) %>% kable()

df_filtered_location %>%
  ungroup() %>%
  arrange( desc(dist_filtered )) %>%
  select( dist_filtered, userid, night ) %>%
  head(20) -> a


a


```

# top 10 raw lat and long points

```{r }

df_filtered_location %>% ungroup() %>% count( longitude, latitude, sort=TRUE) %>% head(10) %>% kable()


```

# distance exploration

```{r distance display}
ggplot(df_filtered_location, aes( latitude )) + geom_histogram()
ggplot(df_filtered_location, aes( longitude )) + geom_histogram()

ggplot(df_filtered_location, aes( dist_original )) +
 geom_histogram() +
 scale_y_log10() +
 xlab( 'range of original distances in km between successive points')


ggplot(df_filtered_location, aes( speed_filtered )) +
 geom_histogram() +
 scale_y_log10() +
 xlab('speed in m/sec between successive filtered points')

ggplot(df_filtered_location, aes( y=speed_filtered, x=night )) +
 geom_violin() + 
 geom_boxplot(width=0.1)+
 scale_y_log10() +
 xlab('speed in m/sec') + 
 ggtitle( 'Range of filtered speeds per night, log scale, outliers dropped')

 
``` 

# Demonstrations of cleaned and uncleaned data

First, the number of points/ night Number with speed > 100 m/s

```{r excess_speed }
df_filtered_location %>% filter( speed_filtered > 100 ) %>% count(sort=TRUE)
df_filtered_location %>% filter( dist_filtered > 100 ) %>% count(sort=TRUE)

#df_filtered_location %>% 
#  ggplot( ) + geom_line( aes( local_time, speed_original  )) +
#  facet_wrap( ~night , scales='free') +
#  ggtitle( "original intersample speed measurements" )
#
#
#df_filtered_location %>% 
#  ggplot( ) + geom_line( aes( local_time, dist_original  )) +
#  facet_wrap( ~night , scales='free') +
#  ggtitle( "original intersample distance measurements" )
#
#df_filtered_location %>% 
#  ggplot( ) + geom_line( aes( local_time, dist_cumul_original  )) +
#  facet_wrap( ~night , scales='free') +
#  ggtitle( "original cumulative distance measurements" )


df_filtered_location %>% 
  inner_join( a, by=c('userid','night')) %>%
  ggplot( ) + geom_line( aes( local_time, speed_filtered  )) +
  facet_wrap( ~userid, scales='free') +
  ggtitle( "Filtered intersample speed measurements" )


df_filtered_location %>% 
  inner_join( a, by=c('userid','night')) %>%
  ggplot( ) + geom_line( aes( local_time, dist_filtered.x  )) +
  facet_wrap( ~userid, scales='free') + 
  ggtitle( "Filtered intersample distance measurements" )



df_filtered_location %>% 
  inner_join( a, by=c('userid','night')) %>%
  ggplot( ) + geom_line( aes( local_time, dist_cumul_filtered  )) +
  facet_wrap( ~userid, scales='free') + 
  ggtitle( "Filtered cumulative distance measurements" )

```

# Heterogeneous Time series exploration: to be continued

```{r time_series}


df_filtered_location %>%  
  as.tsibble( key=id( userid, night), 
             index=local_time, 
             regular=FALSE
             ) %>% 
  { . } ->  ts_location


```


```{r end }

options(warn=0)
```

